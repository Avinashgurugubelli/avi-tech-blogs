
# Summary of Pages 69-79: Storage and Retrieval - Indexing Structures

Pages 69 to 79 of the sources delve into the fundamental concepts of how databases store and retrieve data efficiently, primarily focusing on **indexing structures**. The core idea is that while applications work with data in memory (objects, structs), when this data needs to be stored or sent over a network, it must be encoded into a sequence of bytes. Databases internally decide how to represent this data in bytes, allowing it to be queried, searched, and manipulated.

## The Need for Indexes

A database's primary data is typically stored in a way that prioritizes efficient writes, often by simply **appending to a file (a log)**. This append-only approach is very efficient for writes. However, to **efficiently find a specific value for a particular key**, an additional data structure called an **index** is required. Indexes act as "signposts" or metadata, helping locate the desired data.

*   **Indexes are derived structures**: They are built from the primary data and do not affect the database's content.
*   **Performance Trade-off**: Adding and removing indexes only affects query performance. While they speed up reads, maintaining indexes **slows down writes** because the index must also be updated every time data is written.

## Hash Indexes (Log-Structured Storage)

A simple indexing strategy for data stored as an append-only file is to use an **in-memory hash map**.

*   **Mechanism**: Each key in the hash map is mapped to a **byte offset** in the data file, indicating where its value can be found.
    *   When a new key-value pair is appended, the hash map is updated with the new offset.
    *   To look up a value, the hash map provides the offset, allowing direct seeking and reading from the file.
*   **Analogy (Figure 3-1: Storing a log of key-value pairs in a CSV-like format, indexed with an in-memory hash map)**:
     ![alt text](image.png)
    *   This diagram illustrates how keys in the hash map point to their corresponding values (and their metadata) in the append-only data file.
*   **Durability and Compaction**:
    *   To prevent the log file from growing indefinitely, it is broken into **segments** (when it reaches a certain size and making subsequent writes to new segment file, we can then perform Compaction in these segments.).
    *   **Compaction** is performed on these segments, which means **throwing away duplicate keys and keeping only the most recent update** for each key.
    *   **Merging**: Compaction often merges several segments into a new, smaller file, allowing the old segment files to be deleted. This process happens in a background thread, not interrupting read/write requests.
   *   **Analogy (Figure 3-2: Compaction of a key-value update log)**:
       ![alt text](image-1.png)

   *   Moreover, since compaction often makes segments much smaller (assuming that a
key is overwritten several times on average within one segment), we can also merge
several segments together at the same time as performing the compaction, as shown
in Figure 3-3. Segments are never modified after they have been written, so the
merged segment is written to a new file. The merging and compaction of frozen seg‐
ments can be done in a background thread, and while it is going on, we can still con‐
tinue to serve read and write requests as normal, using the old segment files. After the
merging process is complete, we switch read requests to using the new merged seg‐
ment instead of the old segments—and then the old segment files can simply be
deleted.
*   **Analogy (Figure 3-3: Performing compaction and segment merging simultaneously)**:
![alt text](image-2.png)

*   **Real-world Implementations - Issues Briefly**:
    *   Bitcask (default in Riak) uses this approach, offering high read/write performance if keys fit in RAM.
      *   **File format** 
CSV is not the best format for a log. It’s faster and simpler to use a binary format
that first encodes the length of a string in bytes, followed by the raw string
(without need for escaping).

    *  **Deleting records** 
If you want to delete a key and its associated value, you have to append a special
deletion record to the data file (sometimes called a tombstone). When log seg‐
ments are merged, the tombston
    *   **Crash recovery** rIf the database is restarted, the in-memory hash maps are lost. In principle, you
can restore each segment’s hash map by reading the entire segment file from
beginning to end and noting the offset of the most recent value for every key as
you go along. However, that might take a long time if the segment files are large,
which would make server restarts painful. Bitcask speeds up recovery by storing a snapshot of each segment’s hash map on disk, which can be loaded into mem‐
ory more quickly
    *   **Concurrency control** As writes are appended to the log in a strictly sequential order, a common imple‐
mentation choice is to have only one writer thread. Data file segments are
append-only and otherwise immutable, so they can be read concurrently by mul‐
tiple threads.

*   **Limitations of Hash Table Indexes**:
    *   The **hash table (index) must fit entirely in memory**. This makes it unsuitable for datasets with a very large number of distinct keys that exceed available RAM.
    *   **Range queries (e.g., scanning all keys between X and Y) are inefficient** because hash maps do not preserve key order.

## SSTables and LSM-Trees

To overcome the limitations of hash indexes, especially for large key spaces and efficient range queries, a modified log segment format is used: **Sorted String Tables (SSTables)**.

*   **Core Idea**: The key-value pairs within each segment file are **sorted by key**. Each key appears only once per merged segment file.
*   **Advantages of SSTables**:
    1.  **Efficient Merging (Figure 3-4: Merging several SSTable segments)**:
        *   Merging sorted segments is simple and efficient, similar to the mergesort algorithm. This works even if files are larger than memory.
        *   When merging, if a key exists in multiple segments, the value from the **most recent segment is kept**, and older values are discarded.
        ```
        Segment A (sorted) : [A:1], [C:3], [E:5]
        Segment B (sorted) : [B:2], [C:4], [F:6]
             ↓ Merge (Mergesort-like)
        Output (sorted)    : [A:1], [B:2], [C:4], [E:5], [F:6] (C:4 is newest)
        ```
        ![alt text](image-3.png)
    2.  **Sparse In-Memory Index (Figure 3-5: An SSTable with an in-memory index)**:
        *   You don't need to keep an index of *all* keys in memory. A **sparse index** (e.g., one key for every few kilobytes) is sufficient.
        *   To find a key, the sparse index points to a nearby offset, and then a small sequential scan finds the exact key.
        ```
        SSTable File (sorted by key):
        [handbag: ...]
        [handiwork: ...]  <-- Target Key
        [handsome: ...]

        In-Memory Index:
        Key        Offset
        handbag -> 100
        handsome -> 200

        To find 'handiwork', jump to 100 and scan until 'handiwork' or 'handsome' is found.
        ```
        ![alt text](image-4.png)
    3.  **Compression**: Because data is sorted, it's possible to group records into blocks and compress them before writing to disk, saving space and reducing I/O bandwidth.

*   **Constructing and Maintaining SSTables**:
    *   Incoming writes are first added to an **in-memory balanced tree data structure (memtable)**, such as a red-black/ AVL tree, which keeps keys sorted.
    *   When the memtable reaches a threshold size, it is **efficiently written out to disk as a new SSTable file** (because it's already sorted). Writes continue to a new memtable instance during this flush.
    *   **Read requests** first check the memtable, then the most recent on-disk SSTable, and so on.
    *   A **separate append-only log** is kept on disk for new writes to ensure durability in case of a crash before the memtable is flushed. This log is discarded once the memtable is successfully written to an SSTable.
    *   **Background merging and compaction** combine segment files and discard obsolete values.

*   **LSM-Trees (Log-Structured Merge-Trees)**:
    *   Storage engines that operate on this principle of merging and compacting sorted files are often called **LSM storage engines**.
    *   Examples include LevelDB, RocksDB, HBase, and Cassandra.
    *   **Full-text search engines** like Lucene also use a similar method for storing their term dictionaries, which are essentially key-value structures where the key is a word and the value is a list of document IDs. These mappings are kept in SSTable-like sorted files.
*   **Performance Optimizations**:
    *   For lookups of non-existent keys (which can be slow in LSM-trees as they might require checking all segments), **Bloom filters** are often used. A Bloom filter is a memory-efficient probabilistic data structure that can quickly tell if a key *definitely does not* exist, saving unnecessary disk reads.
    *   Different **compaction strategies** like size-tiered and leveled compaction are used to manage merging and disk space efficiently.

This section highlights how these data structures form the backbone of many modern databases, balancing the needs for efficient writes, reads, and fault tolerance.
